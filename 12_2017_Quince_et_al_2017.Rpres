MtlMicro Meeting: December 2017
Quince et al., 2017

"DESMAN: a new tool for de novo extraction of strains from metagenome" by Quince et al. 2017
========================================================
author: Patricia Tran /MtlMICRO Meeting
date: December 12, 2017
#autosize: true
width: 1440
height: 900


WELCOME!
========================================================
```{r include=FALSE}
knitr::opts_chunk$set(out.height="400px", dpi=120, echo=FALSE)
```


Welcome to the First MtlMicro Meeting!
Thank you all for attending and for the paper suggestions.

This paper was suggested by Nicolas Tromas from the Shapiro Lab. 

ABSTRACT
========================================================

Problem:
Large multi-sample metagenomes are generated, but strain variation results in fragmentary co-assemblies
Therefore, the bins (MAGs) are can be an aggregate of closely related strains.

What DESMAN is:
a tools that identifies variants in core genes and uses co-occurrence across samples to link these variants into haplotypes and abundance profiles

How this was tested:
- in silico
- A complex 50-species 210-genome 96-samples synthetic mock data set and applied to the Tara Oceans Microbiome

The basics...What's a metagenome?
========================================================
A metagenome is a genome reconstructed (metagenome-assembled-genomes, or MAGs, or bin) from all the DNA extracted from an environmental sample.

Overview of the DESMAN pipeline
====================================
![alt text](12_2017_Quince_et_al_2017-figure/Quince_et_al_2017_Figure1.gif)

Figure 1.

Proof of concept
====================================
Tested the DESMAN pipeline on two datasets
- E.coli Outbreak = mock community (simple) + test on real metagenomic dataset (simple, 5 strains, known outbreak strain)
- Complex environmental samples = more representative to what a environmental microbiologist would do. Using the TARA Oceans project. complex mock community + test on real data set (complex TARA Oceans)

Mock Community (Simple)
====================================
1. Binning (using Concoct, or any other de novo binning method)
  + 20 metagenomes --> 19 bins
  + Bins were merged to obtain pangenome
  + ID coding domains --> Assign to contigs --> See Add. Methods *"Identifying core genes in target species"*
    + In short: Download complete E.coli genomes from NCBI --> Use RPS-BLAST to assign to COGS --> ID COGs that are in single copy and have >95% ANI to the known E.coli strains ---> call those "SCGS"

2. Variant detection
  + Map the reads back to the SCGS to obtain sample-specific base frequencies at each positing. Use Likelihood ratio test (statistical test used for comparing the goodness of fit of two statistical models, one of which (the null model) is a special case of the other (the alternative model))
  + If the False Discovery Rate (FDR) is <10^-3 = Classify as a **variant**

3. Strain deconvolution
  + Obtain a certain X of potential variant positions in the Y SCGS
  + (here, 6044 variants in 372 SCGS)
  + **Figure 2a**
    
Mock Community (Cont.)
====================================
    
4. Comparison to existing algorithms
  + Subsample the variants and tested the programs
  + Lineage (O'Brien et al., 2014): Correctly predicted 5 strains, but 2 of them were identical, and matched a known strain. Accuracy was lower than DESMAN (76.32% vs. 99.6%)
  + ConStrains (Luo et al., 2015): Issue of insufficient coverage (despite 37-432 coverage, unlike 10.0 stated in the documentation)

5. Effect of sample number on sample inference:
  + Asking: "How many samples do I need for this to work?"
  + Subsample sample 1 to 64, as long as the relative abundances in the subsamples is ~ that of the 64 sample
  + Run DESMAN on only the subsample
  + **Figure 2b**: SNV error rate increases when sample number <= 30, but average error ~15%, even with just 10 samples. At low sample number, accuracy is very variable --> some strains will be resolved accurately, wherease others are missed completely.

6. Inference of strain abundances:
  + i.e. : What's the frequency of each strain in each sample?
  + Relative frequency = proportion of coverage deriving from each strain, when the reads are mapped unto each MAG. 
  + **2c** : Predicted frequency is almost 1:1 to the real E.coli strain frequency.

Mock Community (Cont.)
====================================

7. Run time: pretty quick, less than 2 hours on 1 core, without parallelization. Scales linearly with sample number.

8. Gene assignment: 
  + i.e. is it good at assigning non-core genes?
  + Take the posterior mean deviance (y value of Figure 2a, for which G < 5) --> Parameters to infer the presence or absence of each genes in each strain. 
  + **Figure 2d**:(what do these colours mean?!)

Testing using the Mock Community
====================================
![](12_2017_Quince_et_al_2017-figure/Quince_et_al_2017_Figure2.gif)
Figure 2

E.coli O104:H4 outbreak
====================================
Applied the algorithm to 53 human feacal samples from the 2011 STEC O104:H4 outbreak

Starting point: 
- unknown number of exact strains present in the population nor their proportions
- We know one of these strains, from cultured isolate that was sequenced.
- Can we use DESMAN to find/predict that particular strain?

A few points:
20 samples were used because they had mean coverage of SCSG (Single core genes) greater than 5. Smaller than that is hard to deconvolute.

E.coli validation of reconstructed strains
====================================
![](12_2017_Quince_et_al_2017-figure/Quince_et_al_2017_Figure3.gif)
Figure 3
a. Two groups: One is low uncertainty, and varies in abundance. The second is high in uncertainty and low abundance.
b. Phylo tree of the strains predicted and the 63 reference genomes = indeed, it's obvious is H3 is probably not a real E.coli strain (very long branch) (Using MAFFT and FastTree)


Note: H4, H1 and H6 are not nested within known reference:

Discussion: Is it novel strains, or just low uncertainty?

The authors then inferences the presence/absence of the other genes in the 3 bins and found that H7 was most closely related to the 0104:H4 outbreak stain, with 91.8% of the inferred genes being identical (!)

Complex Mock Community
====================================
![](12_2017_Quince_et_al_2017-figure/Quince_et_al_2017_Figure4.gif)
Figure 4.
a. No of variants (True Positive, False Positive, False Negative) vs. Cluster (37 on the x axis), 27 of them are TP (red).

b. Haplotype inference accuracy. Err = Mean error rate in SNV predictions on single-copy core genes (SCG)

c. Comparison of the true relative strain frequency and the inferred haplotype frequency across the 96 samples of the complex mock community. E = SNV Error rate

d. Haplotype SNV error vs. gene presence/absence error rate. 

TARA Oceans plankton microbiome survey
====================================
7.2 TB of metagenomic data, 243 samples accross 68 locations from epipelagic and mesopelagic waters globally

- No attemps to bin into MAGs in the original study
- Delmont et al., 2011 : Extracted 957 MAGs from 93 of these samples (Using CONCOCT and ANVI'O)
    + 32 of the most abundant MAGS with 75% of SCG %>% DESMAN
    + 4 Actinobacteria, 6 Bacteroidetes, 1 *Candidatus Marinimicrobia*, 1 Chloroflexi, 3 Euryarchaeota and 17 Proteobacteria

Figure 5
====================================
![](12_2017_Quince_et_al_2017-figure/Quince_et_al_2017_Figure5.gif)

Fig. 5 a. No. of haplotypes is negatively correlated with genome length. 
*Greater diversity occurs in streamlined genomes --> important process for diversity in plankton microbiome*

b. Genomes that are streamlined (blue <1Mpb) have a stronger positive relationship between genome divergence (how different two genomes are) and SCG (singel copy gene) nucleotide divergence. 

_Difference in the way **strain divergence** and **niche partionning** impacts genome as genome length varies_

_If genome is large + complex metabolism --> niche differentiation required less changes in the genome with changes in gene expression levels in order to drive ecological difference_

_On the other hand, if the genome is small, a large genome change is required for strain generation and adapation to new niches_

Figure 6
====================================
![](12_2017_Quince_et_al_2017-figure/Quince_et_al_2017_Figure6.gif)
Haplotypes correlated with geographic location may suggest local adaptation.

Open-ended discussion
====================================
Example questions to discuss:

- De novo vs. reference based?
- What is a metagenome? What is a species? Is it an operational terms? (any more useful than OTU?)
- What are the caveats of using these terms?
- In which scenarios would deconvoluting MAGs be useful? For what kind of questions?
- The method described assumed that we start with a co-assembly, and bin those contigs. Should we even use a co-assembly to begin with?
- Would this method can useful in your research?
- What binning algorithm do you use for your research? What are the pros and cons you have encountered?

Some key terms
====================================
>"We developped a full **Bayesian Model**, fitted by a **Markov Chain Monte Carlo (MCMC)** **Gibbs sampler**, to learn the strain frequencies, their **haplotypes** and allso sequencing error rates. To improve **convergence**, we initialize the Gibbs sampler using a **non-negative matrix factorisation** (NMF), or more properly **non-negative tensor factorisation (NTF)**, a method from machine learning that is equivalent to the **maximum likelihood solution**" 

(Second page of PDF, Quince et al. 2017)

Definitions
====================================
**Bayesian Model**: a graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph.

**MCMC**: A method to sample from, and to computing expectations with high-dimensional probability distributions. Check out this video <https://www.youtube.com/watch?v=12eZWG0Z5gY> for an intro to it.

**Gibbs Sampler**: One class of algorithm (a series of well defined steps to achieve an outcome/method) that consist of generate posterior samples by sweeping through each variable (or block of variable) to sample from its conditional distribution with the remaining variables fixed to their current values (Yildirim, 2012: Bayesian Inference: Gibbs Sampling, U. of Rochester)

**Convergence**: When the sample values have the same distribution as if they were sampled from the true posterior joint distribution (Yildirim, 2012)

Non-negative matrix factorisation:

Non-negative tensor factorisation:

**Maximum-likelihood**: The process of finding the value of one or more parameters of a model, given observations by finding the parameters values that maximized the likelihood of making the observations given the parameters. 

Discussion : Technical improvements stated by the authors
====================================
1. Position-dependent error rates: give the high error rates on Illumina sequencers
2. Models that do not assume independence across variant positions by combining info from the co-occurrence of variances in the same read with the modelling of strain abundances across multiple samples: given the expected popularity of single molecule long sequencers (e.g. Nanopore)
3. A more "principled" method than examining posterior mean deviance: e.g. through Bayesian non-parametric (e.g. Dirichlet process prior for the strain frequencies)
4. Variational Bayesian approach to obtain lower bound of ML and used to distinguish between models






